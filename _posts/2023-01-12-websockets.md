---
layout: post
title: "WebSockets: a comparison between open-source technologies"
author: matteo.ricci
categories: [ Software Development ]
image: assets/images/websockets/0.png
excerpt: "Benchmarking  WebSocket server performaces through a simple experiment"
---

## Introduction
WebSocket is a communication protocol that supplies two-way communication between client and server.
When a client starts a connection with a WebSockets server, this latter can push messages to all the connected clients. 
That is in contrast with the **HTTP** client-server model, where a server cannot initiate a 
message transfer to a client but can only respond to an **HTTP** request started by a client. 
Further, by keeping the communication channel open, a WebSockets server allows the simultaneous sending and receiving 
of messages among the connected clients through the server itself. 

<center>
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/websockets/1.png" width="400">
    <br>
    <em> A WebSocket server receives messages and notifies subscribers (Image generated by the author with E-Dell)</em>
</center>

In this work, we employed state-of-the-art, open-source, **JVM** technologies to assess the performances of a WebSocket server under progressive system overload. The focus of the study is comparing two event-loop models: **Spring Webflux** and **Ktor**.
Nonetheless, some room for a didactical discussion will be left, explicitly showing the performance gap achieved by employing a thread-per-request Spring Boot WebSockets engine.

## Methods
Our primary goal is to think about a minimal system involving WebSockets communication so that all the side effects (e.g., latency) are negligible. 
That is done by implementing an echo server, in which both the server and client run on a local machine.
This system forwards to the client the message the client has just sent. Such a simple toy model allows us to apply progressive system overload by increasing the number of concurrent users exchanging messages with the echo server. 
The run is carried out with **Apache JMeter** and is structured in the following way:
- We consider an initial chunk of 50 users connected to the echo server. 
- We keep increasing connections in chunks of 200 up to the **maximum overload state**, with 2000 concurrent users.
- We keep the system in the maximum overload state for 3 minutes.
- We start dropping out connections in chunks of 400 until the number of active connections drops to 0.
If any communication errors happen, the existing connection will be closed, kicking out the user from the echo server.

Separate runs for each of the two event-loop technologies are performed. The following quantities are of interest:
- raw data describing the server response over time.
- the actual number of concurrent users over time (may be different from the planned one, since if a communication error happens, the connection will be closed).
- the percentage of requests processed with an error outcome.
- the number of exchanged bytes between the concurrent clients and the echo server.
- the echo-server throughput.

Finally, we perform the simulation using a **5 kb** text sample to exchange between the client and server. 
In particular, to simulate a real-life scenario, we require every user to exchange messages with the echo server according to a time-based Gaussian distribution. 

<center>
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/websockets/3.png" width="400">
    <br>
    <em> Gaussian distribution of the delay every user exchanges messages with. The image is generated by the author.</em>
</center>

The Gaussian distribution means value and standard deviation are 5 and 2 seconds. 
This means that the probability every user waits between 1 and 9 seconds before exchanging the second message with the echo server is almost maximized (**95%**).

## Results and Conclusions
In this section, we present the results achieved using the methodology explained in the previous section.
<center>
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/websockets/ktor_webflux_2000.png" width="1000">
    <br>
    <em> In the top panel the number of connected users during the run is presented. In the bottom panel, the comparison between the even-loop-based echo server performances is shown. The Image produced by the author using the software R. </em>
</center>
For both the technologies employed, none of the incoming requests have an error outcome. 
Also, the amount of exchanged Kb per second and the server throughputs are very similar: so far seems that both technologies are pretty performant. 

However, let's try to deeper investigate what we have achieved to have a more solid understanding of what happened.
The dotted lines represent the raw data of the server response over time, directly from **Apache JMeter** without any post-processing. 
As we can see, randomly distributed higher peaks corresponding to fluctuations up to 20 ms are present. 
To understand the relative impact of those peaks with respect to the averagely observed values, 
we made an interpolation of the raw data using the software **R**. The results of this procedure are shown with solid lines. 
Looking at those lines, it is clear that, regardless of fluctuations, 
the average response time for **Ktor** lies slightly below 
the one observed for **WebFlux**, thus providing a better understanding of what is going on with the 
overall simulation parameters in the table. 
Indeed, the interpolated trends reflect the slightly higher server throughput and the number of bytes 
exchanged with the concurrent clients achieved by **Ktor**. Most importantly, it indicates the low impact of the observed fluctuations along the simulation, showing that both technologies ensure stability during the run. 

The code snippets for **Ktor** and **Webflux** echo servers are listed below.

<p style="margin : 0; padding-bottom:0;"> <em> Ktor WebSocket server.</em> </p>
```kotlin
fun Application.configureSockets() {
    install(WebSockets) {
        pingPeriod = Duration.ofSeconds(10)
        timeout = Duration.ofSeconds(15)
        maxFrameSize = Long.MAX_VALUE
        masking = false
    }
    routing {
        webSocket("/ktor") {
            for (frame in incoming) {
                frame as? Frame.Text ?: continue
                val userMessage = frame.readText()
                send(userMessage)
            }
        }
    }
}
```


<p style="margin : 0; padding-bottom:0;"> <em> WebFlux WebSocket server.</em> </p>
```java
@Component
public class ReactiveServerWebSocketHandler implements WebSocketHandler {

    @Override
    public @NotNull
    Mono<Void> handle(@NotNull WebSocketSession session) {
        return session.send(session.receive()
                .map(WebSocketMessage::getPayloadAsText)
                .map(session::textMessage)
        );
    }

    @Override
    public @NotNull
    List<String> getSubProtocols() {
        return WebSocketHandler.super.getSubProtocols();
    }
}
```


After this digression, we can conclude that **Ktor** has performed slightly better, although we have observed a very 
high-quality performance for **WebFlux**. 

However, we should point out that the presented results rely on a single run, which means they lack statistical ensemble wideness. 
In other words, to give a better outcome about the best-performing, we should carry out many runs and consider the average results from each of them.

## A Didactical Digression
It is interesting to carry out the same experiment by adopting a thread-per-request model, to observe the performance gap with the event-loop models presented so far. 
We can build up the same echo server employing this time the standard WebSocket engine provided by Spring Boot. 
Carrying out the same experiment as explained, we achieved the following results.

<center>
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/websockets/normal-results.png" width="1000">
    <br>
    <em> In the top panel the number of connected users during the run is presented. In the bottom panel, the comparison between the even-loop-based echo server performances is shown. Image produced using raw data from JMeter. </em>
</center>

In this case, the echo server can't handle more than roughly 100 concurrent connections, as indicated by the overall results parameters of the run. In particular, an error percentage of around 13% and a lower server throughput (16.5 sec) are detected. 
Also, looking at the server response over time, we can see every peak seems to appear at each time instant a new connections chunk comes in. 
That, in turn, forces the echo server -unable to deal with such an overload- to rebalance, dropping a large number of existing connections, as shown in the top panel of the last picture.  

What observed, clearly indicates that the employed **thread-per-request** model can't handle a high number of concurrent requests, 
suggesting that having a single thread taking care of the whole processing of a single request is not optimal when the number of concurrent clients becomes relatively high. That is quite intuitive thinking about the finite size of the thread pool the **JVM** has. 

<p style="margin : 0; padding-bottom:0;"> <em> Thread-per-request WebSocket server.</em> </p>
```java
@Component
public class ServerWebSocketHandler extends TextWebSocketHandler {

    @Override
    public void afterConnectionEstablished(WebSocketSession session) {
        session.setTextMessageSizeLimit(20000000);
    }

    @Override
    public void handleTextMessage(WebSocketSession callerSession, @NotNull TextMessage toForward) throws IOException {
        String content = toForward.getPayload();
        callerSession.sendMessage(new TextMessage(content));
    }
}
```

In conclusion, we created an echo-server model to assess the performances of state-of-the-art event loop technologies, 
showcasing their high-level performances. 
On the other hand, we have created the same echo server employing a thread-per-request engine, showing the inadequacy of this model to handle a higher amount of concurrent requests. 

